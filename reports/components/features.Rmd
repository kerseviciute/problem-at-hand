### Run `r gsub(x = run, pattern = 'run', replacement = '')`

#### Epochs {.unlisted .unnumbered}

```{python}
prefix = f'{r.prefix}/{r.run}'

data = pd.read_pickle(f'{prefix}/mne_good_channels.pkl')
events = pd.read_pickle(f'{prefix}/events.pkl')
feature_matrix = pd.read_csv(f'{prefix}/feature_matrix.csv', index_col = 0)
feature_info = pd.read_csv(f'{prefix}/feature_key.csv')
key = pd.read_csv(f'{prefix}/event_key.csv')

feature_length = r.feature_length
min_freq = r.min_freq
max_freq = r.max_freq
freq_step = r.freq_step
nperseg = r.nperseg
relative = r.relative

example_event = events[ events['EventID'] == 'Event3' ]
event = example_event.iloc[0]

normalized_events = normalize_events(events, feature_length = feature_length)
example_normalized_event = normalized_events[normalized_events['OriginalEventID'] == event['EventID']]
```

Firstly, the events were cropped into epochs of equal length of `r feature_length` seconds. If it was impossible
to divide an event into equal time segments, the event was cropped from both start and end (e.g., an event of
`r round(feature_length * 12 + feature_length / 2, 2)` seconds would be shortened by
`r round(feature_length / 4, 2)` seconds from both start and end in order to make it `r round(feature_length * 12, 2)`
seconds long, a time interval which is divided into exactly 12 segments of `r feature_length` seconds).

**Table 2.** Example event **before** splitting into epochs.

```{r, include = TRUE}
data.table::data.table(py$example_event) %>% showTable()
```

**Table 3.** Example event **after** splitting into epochs.

```{r, include = TRUE}
data.table::data.table(py$example_normalized_event) %>% showTable()
```

```{python, include = TRUE}
event_data = data.copy().crop(tmin = event['Start'], tmax = event['End'])
first_start = example_normalized_event.iloc[0]['Start']
colors = ['white', 'gray']
average = np.mean(event_data.get_data(), axis = 0)
limit = np.max(np.abs(average))
limit = limit + limit * 0.25
target_event = example_normalized_event.iloc[int(len(example_normalized_event) / 2)]['EventID']

plt.plot(event_data.times + first_start, average, c = 'black', linewidth = 0.5)

for i, epoch in example_normalized_event.iterrows():
    if epoch['EventID'] == target_event:
        color = 'blue'
    else:
        color = colors[i % 2]
    plt.axvspan(epoch['Start'], epoch['End'], facecolor = color, alpha = 0.25)

plt.xlim(np.min(example_normalized_event['Start']), np.max(example_normalized_event['End']));
plt.ylim(-limit, limit);
plt.show()
```

**Figure 2.** Average signal of all channels during the example event (divided into epochs). Only the epoch marked
blue is analyzed further.

#### Features: power spectral density within frequency bands {.unlisted .unnumbered}

The power spectral density (PSD) was estimated within each epoch. Welch's method was used, with the length
of each segment (`nperseg`) of `r nperseg`. The frequency range from `r min_freq` Hz to `r max_freq` Hz was
divided into `r freq_step` Hz width bands (e.g., `r min_freq`-`r min_freq + freq_step` Hz,
`r min_freq + freq_step`-`r min_freq + freq_step * 2` Hz, and so on), and the area under the PSD curve
was calculated for each frequency band using the Simpson's rule.

```{python}
# Select a random channel
np.random.seed(42)
channel = data.ch_names[np.random.randint(0, len(data.ch_names), 1)[0]]

# Get the event of interest
event = example_normalized_event[example_normalized_event['EventID'] == target_event].iloc[0]

# Get event data
event_data = data.copy().crop(tmin = event['Start'], tmax = event['End']).get_data(picks = [channel])[0]

# Define the frequency ranges
freq_ranges = np.arange(min_freq, max_freq, freq_step)

# Calculate the PSD
sfreq = data.info['sfreq']
freqs, psd = welch(event_data, sfreq, nperseg = nperseg)
```

```{python, include = TRUE}
colors = ['blue', 'red']

plt.plot(freqs, psd, linewidth = 0.75, color = 'black');

for i, low in enumerate(freq_ranges):
    color = colors[i % 2]
    high = low + freq_step
    idx_band = np.logical_and(freqs >= low, freqs <= high)
    plt.fill_between(freqs, psd, where = idx_band, color = color, alpha = 0.1);

plt.xlim(min_freq, max_freq);
plt.ylim(0, np.max(psd) + np.max(psd) * 0.05);
plt.title(f'Power spectral density in channel {channel}');
plt.show();
```

**Figure 3.** Power spectral density within frequency bands. Interchanging blue and orange colors mark
different features.

#### Extracted features {.unlisted .unnumbered}

```{r}
feature_key <- fread(glue('{prefix}/{run}/feature_key.csv'))
event_key <- fread(glue('{prefix}/{run}/event_key.csv'))
feature_key <- feature_key[ Frequency < 50 - freq_step + 1 | Frequency > 70 ]

feature_matrix <- fread(glue('{prefix}/{run}/feature_matrix.csv'))
feature_matrix <- feature_matrix[ FeatureID %in% feature_key[ , FeatureID ] ]
feature_matrix_rownames <- feature_matrix[ , FeatureID ]

stopifnot(all(feature_matrix[ , FeatureID ] %in% feature_key[ , FeatureID ]))
stopifnot(all(colnames(feature_matrix)[ -1 ] %in% event_key[ , EventID ]))
stopifnot(nrow(feature_matrix) == nrow(feature_key))
stopifnot(ncol(feature_matrix) - 1 == nrow(event_key))

x <- feature_matrix[ , -1 ] %>%
  setDF(rownames = feature_matrix_rownames) %>%
  as.matrix() %>%
  log()

p1 <- data.table(Bandpower = as.numeric(x)) %>%
  ggplot() +
  geom_histogram(
    aes(x = Bandpower, y = after_stat(density)),
    fill = 'white',
    color = 'black',
    binwidth = 0.25,
    linewidth = 0.25
  ) +
  xlab('Absolute band power, log') +
  ylab('Density') +
  theme_bw(base_size = 9) +
  theme(panel.grid = element_blank())

x <- x %>%
  t() %>%
  scale(center = TRUE, scale = FALSE) %>%
  t()

p2 <- x %>%
  apply(1, \(x) { sd(x)^2 %>% exp() %>% -1 %>% sqrt() }) %>%
  data.table(CV = .) %>%
  ggplot() +
  geom_histogram(
    aes(x = CV, y = after_stat(density)),
    fill = 'white',
    color = 'black',
    binwidth = 0.025,
    linewidth = 0.25
  ) +
  ylab('Density') +
  xlab('Coefficient of variation (CV)') +
  theme_bw(base_size = 9) +
  expand_limits(x = 1) +
  theme(panel.grid = element_blank())

event_pca <- prcomp(t(x), rank. = 2)$x %>%
  as.data.table(keep.rownames = 'EventID') %>%
  merge(event_key) %>%
  .[ , EventType := make.names(Event) %>% factor() %>% relevel(ref = 'X0') ] %>%
  .[ ]

feature_pca <- prcomp(x, rank. = 2)$x %>%
  as.data.table(keep.rownames = 'FeatureID') %>%
  merge(feature_key) %>%
  .[ , Frequency := factor(Frequency) ] %>%
  .[ ]

feature_annotation <- feature_pca[ , list(PC1 = median(PC1), PC2 = median(PC2)), by = Frequency ]

p3 <- event_pca %>%
  ggplot() +
  geom_point(aes(x = PC1, y = PC2, color = EventType, fill = EventType), size = 1, alpha = 0.75) +
  theme_bw(base_size = 9) +
  theme(panel.grid = element_blank()) +
  xlab('1st principal component') +
  ylab('2nd principal component') +
  theme(legend.position = 'top') +
  scale_color_discrete(name = 'Event') +
  scale_fill_discrete(name = 'Event')

p4 <- feature_pca %>%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point(aes(fill = Frequency, color = Frequency), size = 1, alpha = 0.75) +
  theme_bw(base_size = 9) +
  theme(panel.grid = element_blank()) +
  xlab('1st principal component') +
  ylab('2nd principal component') +
  theme(legend.position = 'none') +
  geom_label_repel(
    data = feature_annotation,
    aes(x = PC1, y = PC2, label = Frequency),
    min.segment.length = 0,
    inherit.aes = FALSE,
    size = 2.5
  )
```

The power line frequencies (50-70 Hz) were removed from extracted feature data. For the remainder of the
features, coefficient of correlation was calculated. For the data that is log-normally distributed an estimate
of coefficient of variation is defined as:

$$\hat{cv} = \sqrt{e^{s^{2}_{ln}} - 1}$$

where $s_{ln}$ is the standard deviation of the data after the natural log transformation.

Principal component analysis was performed to search for both event and feature clustering.

```{r, include = TRUE, fig.height = 7.2}
ggarrange(
  p1, p2, p3, p4,
  nrow = 2,
  ncol = 2,
  labels = letters[ 1:4 ],
  font.label = list(size = 11)
)
```

**Figure 4.**
**(a)** The distribution of log-transformed feature data.
**(b)** The distribution of coefficient of variation of log-transformed centered feature data.
**(c)** The first two principal components of event PCA. The colors represent different events.
**(d)** The first two principal components of feature PCA. The colors and numbers represent different frequencies.
